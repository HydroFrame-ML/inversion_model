{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e7bbf7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2231577/2651136139.py:15: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import parflow as pf\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "from torch import nn\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from src.datapipes import (\n",
    "    gen_normalized_ds_minmax,\n",
    "    gen_normalized_ds_zscale,\n",
    "    create_dataloader\n",
    ")\n",
    "from src.models import UNet\n",
    "from src.loss import mse_loss\n",
    "from src.train import train_epoch, save_experiment, load_experiment\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93d844f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DTYPE = torch.float32\n",
    "DTYPE_NAME = \"torch.float32\"\n",
    "torch.set_default_dtype(DTYPE)\n",
    "mod_num = '5_retrain'\n",
    "mode = \"w\"\n",
    "header = True\n",
    "unet_layers = 4\n",
    "\n",
    "write_dir = \"/home/at8471/c1_inversion/ml_training/outputs/logK_css_pme75_ani10\"\n",
    "model_run_name = f\"css_pme75_ani10_ens_model{mod_num}\"\n",
    "yaml_flag = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b09def5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e08bc39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "in_vars = ['pme', 'wtd', 'topo_index', 'elev', 'slope_x', 'slope_y']\n",
    "out_vars = ['ksat']\n",
    "\n",
    "ds = xr.open_dataset('../inputs/conus_steady_state_parflow.nc').load() #to get topo_index and elev vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d950f869",
   "metadata": {},
   "outputs": [],
   "source": [
    "lake_mask = np.flip(pf.read_pfb('/home/at8471/c1_inversion/ml_training/inputs/c1_great_lakes_masked.pfb').squeeze(),axis = 0)\n",
    "ksat_ens = xr.open_dataset('../inputs/ani10_ef_PmE75_wtd.Ksat.sx.sy.pme.zarr', engine='zarr')\n",
    "ksat_ens['topo_index'] = ds['TopoIndex']\n",
    "ksat_ens['elev'] = ds['elev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "158e10a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ksat_ens['ksat'] = np.log(ksat_ens['ksat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ea815cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_ksat_ds = gen_normalized_ds_zscale(ksat_ens)\n",
    "norm_ksat_ds['lake_mask'] = xr.DataArray(lake_mask, dims = ('y','x'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5d3af31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43, 1, 28, 14, 36, 12, 0, 27, 47, 7]\n",
      "[33, 34, 20, 49, 5, 38, 11, 23, 40, 15]\n",
      "[2, 3, 4, 6, 8, 9, 10, 13, 16, 17, 18, 19, 21, 22, 24, 25, 26, 29, 30, 31, 32, 35, 37, 39, 41, 42, 44, 45, 46, 48]\n"
     ]
    }
   ],
   "source": [
    "#splitting out training/test/validation\n",
    "reserve_frac = 0.4 #leave out for valid/test #60,20,20\n",
    "ind_list = list(range(0,50))\n",
    "random.shuffle(ind_list)\n",
    "n_reserved = int(len(ind_list)*reserve_frac)\n",
    "valid_inds = list(ind_list[0:(n_reserved//2)])\n",
    "test_inds = list(ind_list[(n_reserved//2):n_reserved])\n",
    "train_inds = list(set(np.arange(0, 50)) - set(valid_inds) - set(test_inds)) #sets in py unique, can't contain dups, what remains is the unique inds for training data\n",
    "\n",
    "train_inds_str = \", \".join([str(number) for number in train_inds])\n",
    "valid_inds_str = \", \".join([str(number) for number in valid_inds])\n",
    "test_inds_str = \", \".join([str(number) for number in test_inds])\n",
    "\n",
    "print(valid_inds)\n",
    "print(test_inds)\n",
    "print(train_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8eeb2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_ksat_ds_train = norm_ksat_ds.isel(n=train_inds)\n",
    "norm_ksat_ds_valid = norm_ksat_ds.isel(n=valid_inds)\n",
    "norm_ksat_ds_test  = norm_ksat_ds.isel(n=test_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e48ea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = 56 #base channels related to UNET, from initial to hidden state (1st step 16 dim)\n",
    "max_epochs = 250\n",
    "width = 64\n",
    "overlap = 26\n",
    "batch_size = 512\n",
    "num_workers = 2 #24 #parallel loading, doesn't really help because of how set up (maybe)\n",
    "learning_rate = \"NONE\" #tried .01 also #start with .0001\n",
    "max_lr = .01\n",
    "base_lr = .0001\n",
    "step_size = 12040 #tried 5, 25 also\n",
    "gamma = \"NONE\" #tried .1, .5 also\n",
    "lambda_val = 2\n",
    "kernel = 5\n",
    "input_dims = {'y': width, 'x': width, 'n': 1}\n",
    "input_overlap = {'y': overlap, 'x': overlap}\n",
    "\n",
    "phase = \"two_phase\"\n",
    "loss_func = \"F.mse_loss\"\n",
    "act_func_choice = \"nn.Tanh\"\n",
    "optimizer_choice = \"AdamW\" #SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0684150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = create_dataloader(\n",
    "    [norm_ksat_ds_train, ], \n",
    "    in_vars, out_vars,\n",
    "    input_dims, input_overlap, batch_size, \n",
    "    num_workers=num_workers, dtype=DTYPE, augment_bool = True\n",
    ")\n",
    "\n",
    "vdl = create_dataloader(\n",
    "    [norm_ksat_ds_valid,], \n",
    "    in_vars, out_vars,\n",
    "    input_dims, input_overlap, batch_size, \n",
    "    num_workers=num_workers, dtype=DTYPE, augment_bool = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc8fb64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tedl = create_dataloader(\n",
    "    [norm_ksat_ds_test,], \n",
    "    in_vars, out_vars,\n",
    "    input_dims, input_overlap, batch_size, \n",
    "    num_workers=num_workers, dtype=DTYPE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbd15c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x,y = next(iter(dl)) #check if the dataloaders worked\n",
    "# x.shape\n",
    "# y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df25c583",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(in_vars, out_vars, activation=nn.Tanh, base_channels=bc)#tanh can be better for regression problems\n",
    "model = model.to(DTYPE)\n",
    "model = model.to(DEVICE)\n",
    "loss_fun = mse_loss\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=max_lr) #start with AdamW, right now stochastic grad descent, (better in long run but you need to wait a really long time, not get stuck in local min)\n",
    "#scheduler = lr_scheduler.StepLR(opt, step_size=step_size, gamma=gamma)\n",
    "#scheduler = lr_scheduler.OneCycleLR(opt, max_lr=max_lr, epochs = max_epochs, steps_per_epoch = batch_size, three_phase = False)\n",
    "scheduler = lr_scheduler.CyclicLR(opt, base_lr = base_lr, max_lr = max_lr, step_size_up = step_size, step_size_down = step_size, last_epoch = -1, cycle_momentum = False) #302 training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d129603",
   "metadata": {},
   "outputs": [],
   "source": [
    "if yaml_flag ==1:\n",
    "    experiment_config = load_experiment(f\"{write_dir}/{model_run_name}.yml\")\n",
    "    model.load_state_dict(torch.load(experiment_config[\"weights_file\"],map_location=torch.device(DEVICE)))\n",
    "    min_valid_loss = experiment_config[\"last_loss\"]\n",
    "else:\n",
    "    experiment_config = {\n",
    "        \"data_config\": {\n",
    "            \"dtype\": DTYPE_NAME,\n",
    "            \"random_seed\": seed,\n",
    "            \"train_inds\": train_inds_str,\n",
    "            \"valid_inds\": valid_inds_str,\n",
    "            \"test_inds\": test_inds_str,\n",
    "            \"input_vars\": in_vars,\n",
    "            \"output_vars\": out_vars,\n",
    "            \"width\": width,\n",
    "            \"overlap\": overlap, \n",
    "            \"batch_size\": batch_size, \n",
    "            \"num_workers\": num_workers, \n",
    "            \"augment_bool\": \"True\", \n",
    "            \"unet_layers\":unet_layers\n",
    "        },\n",
    "        \"model_config\": {\n",
    "            \"max_epochs\": max_epochs,\n",
    "            \"activation_func\": act_func_choice, \n",
    "            \"loss_func\": loss_func,\n",
    "            \"optimizer\": optimizer_choice,\n",
    "            \"input_size\": len(in_vars),\n",
    "            \"output_size\": len(out_vars),\n",
    "            \"base_channels\": bc, \n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"max_lr\":max_lr,\n",
    "            \"base_lr\": base_lr,\n",
    "            \"three-phase\": phase,\n",
    "            \"step_size\":step_size, \n",
    "            \"gamma\":gamma,\n",
    "            \"lambda\":lambda_val, \n",
    "            \"kernel\":kernel\n",
    "        },\n",
    "    }\n",
    "    save_experiment(config = experiment_config,output_dir = write_dir,name=model_run_name, model = None, valid_loss = None, metrics = None)\n",
    "    min_valid_loss = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f418f425",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "for e in (bar := tqdm(range(max_epochs))):\n",
    "    print(f\"starting epoch {e}\")\n",
    "    model.train()\n",
    "    train_loss.append(train_epoch(model, dl, opt, loss_fun,lmbda=lambda_val,func=F.mse_loss, train=True, device=DEVICE))\n",
    "    model.eval()\n",
    "    valid_loss.append(train_epoch(model, vdl, opt,loss_fun,lmbda=lambda_val,func=F.mse_loss, train=False, device=DEVICE))\n",
    "    bar.set_description(f'{train_loss[-1]:.4f}')\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    losses = {'train_loss': train_loss, 'valid_loss': valid_loss}\n",
    "    loss_df = pd.DataFrame.from_dict(losses)\n",
    "    \n",
    "    save_experiment(config = experiment_config,output_dir = write_dir,name=model_run_name,metrics=loss_df, mode = mode, header = header)\n",
    "    \n",
    "    if min_valid_loss > valid_loss[e]:\n",
    "        print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss[e]:.6f}) \\t Saving The Model')\n",
    "        min_valid_loss = valid_loss[e]\n",
    "        print(min_valid_loss)\n",
    "        save_experiment(config = experiment_config,output_dir = write_dir,name=model_run_name,model = model,valid_loss = min_valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c01350",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "it = iter(tedl)\n",
    "x, y = next(it)\n",
    "x = x.to(DEVICE)\n",
    "yy = model(x).squeeze().detach().cpu().numpy()\n",
    "y = y.squeeze().detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014cbe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes= plt.subplots(1, 3, dpi=200, sharex=True, sharey=True)\n",
    "\n",
    "b= 14\n",
    "vmin = y[b].min()\n",
    "vmax = y[b].max()\n",
    "\n",
    "m = axes[0].imshow(y[b], vmin=vmin, vmax=vmax)\n",
    "axes[0].set_title('ParFlow')\n",
    "axes[0].set_xticks([])\n",
    "axes[0].set_yticks([])\n",
    "#plt.colorbar(m, shrink=0.3)\n",
    "\n",
    "m = axes[1].imshow(yy[b], vmin=vmin, vmax=vmax)\n",
    "axes[1].set_title('UNet')\n",
    "plt.colorbar(m, shrink=0.3)\n",
    "\n",
    "m = axes[2].imshow(y[b]-yy[b], vmin=-1, vmax=1, cmap='Spectral')\n",
    "axes[2].set_title('Difference')\n",
    "plt.colorbar(m, shrink=0.3)\n",
    "#plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfa173d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emulator",
   "language": "python",
   "name": "emulator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
